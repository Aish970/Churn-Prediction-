# ğŸ“‰ Customer Churn Prediction

## ğŸ“Œ Overview
Customer churn â€” when users stop using a companyâ€™s product or service â€” presents a major risk to business revenue and growth. This project aims to **predict churn behavior** using various machine learning models, enabling proactive customer retention strategies.

## ğŸ‘¥ Team Members
- Aishwarya Bhethanabotla  
- Samiksha Sunil Dhemse  
- Hemant Pillala  
- Neelaveni Ushakela  

## ğŸ§  Methodology

### ğŸ” Problem Statement
Understanding customer churn is critical to minimizing losses and improving user retention. We analyze customer usage and profile data to identify churn predictors and classify customers accordingly.

### ğŸ“Š Dataset Features
- **Call Failure**: Number of failed calls  
- **Complains**: Whether the customer has filed complaints  
- **Subscription Length**: Duration of subscription  
- **Charge Amount**: Total charge amount  
- **Seconds of Use / Frequency of Use / Frequency of SMS**  
- **Distinct Called Numbers / Age Group / Tariff Plan / Status / Age**  
- **Customer Value / FN / FP**  
- **Churn** (Target): 1 = churned, 0 = retained

## âš™ï¸ Data Preprocessing
- **Dropped Redundant Features**: `Charge Amount`, `Frequency of SMS`  
- **Dropped Irrelevant Features**: `Age`, `Subscription Length`  
- **Log Transform**: Applied to `Customer Value`  
- **Interaction Features**: 
  - `Customer_Value_Complains` = Customer Value Ã— Complains  
  - `Usage_Intensity` = Frequency of Use Ã· Seconds of Use  
- **One-Hot Encoding**: For categorical columns (`Tariff Plan`, `Age Group`)  
- **Scaling**: StandardScaler applied  
- **Class Imbalance**: Addressed with SMOTE after train-test split  

## ğŸ¤– Models Trained
- **Logistic Regression**  
- **Decision Tree**  
- **Random Forest**  
- **XGBoost**  
- **Naive Bayes**  
- **K-Nearest Neighbors (KNN)**  
- **Support Vector Machine (SVM)**  
- **Artificial Neural Network (ANN)**  
- **Gradient Boosting**  
- **AdaBoost**

## ğŸ† Best Performing Models
- **ğŸ… Random Forest**: Highest accuracy and ROC-AUC  
- **ğŸ¥ˆ XGBoost**: Strong performance with high recall  
- **ğŸ‹ï¸ ANN**: Strong in capturing feature interactions  
- Simpler models like **Logistic Regression** and **Naive Bayes** served as baselines

## ğŸ“ˆ Evaluation Metrics
- Accuracy  
- Precision  
- Recall  
- F1-Score  
- ROC-AUC  
- Confusion Matrices and Performance Plots  

## ğŸ“Œ Conclusion
Random Forest emerged as the top-performing model, followed by XGBoost and ANN. Future improvements include more refined hyperparameter tuning, advanced feature engineering, and expanded evaluation on real-time datasets.

## ğŸ“ Dependencies
- Python 3.x  
- scikit-learn  
- XGBoost  
- pandas  
- numpy  
- matplotlib / seaborn  
- imbalanced-learn  

## âœ… To Run
1. Clone this repo  
2. Install dependencies  
3. Run `main.py` or the corresponding notebook to see training and evaluation outputs

---

Feel free to fork, explore, and enhance the model. â­
